This is the code for the paper "Unsupervised learning of digit recognition using spike-timing-dependent plasticity" .

Paper link: http://journal.frontiersin.org/article/10.3389/fncom.2015.00099/abstract#
Original Code link: https://github.com/peter-u-diehl/stdp-mnist

## Installation

In order to get the code run properly, installation should follow the following steps:

  -directly clone the repository from [github-brian](https://github.com/brian-team/brian).

  -Following the installation instruction form the link above, using `python setup.py install`

  -locate the directory where you download the brian, then append the library path within python path
  ```python
  import sys
  sys.path.append('$add-absolute-address-here')
  ```

## Usage
make sure to use the specific conda environment.
```bash
conda activate brian1
```
### Testing
Make sure the test/evaluation flag is `test_mode = True`, then run the command line.
```bash
python Diehl&Cook_spiking_MNIST.py
```

```bash
python Diehl&Cook_MNIST_evaluation.py
```
### Training

I change the clock rate under `b.set_global_preferences` to 0.1ms
### Neuron Configurations:
```python
refrac_e = 5 * b.ms   # Here is unchanged
refrac_i = 2. * b.ms
```
here the refactory period is unchanged since the fabricated neuron range is 2us to 21ms. Since the RC constant is .22ms to 10ms, change neuron_equation_e/i, change two time constants to .22ms and 10ms

As discussed in the original paper, authors claimed that "Increasing the time constant of excitatory neuron membrane Potential to 100ms (from 10~20ms that typically observed in biological neuron), can significantly increase the classification accuracy.

### Synpase configurations:
```python
```



## Potential questions about the code

First of all, I have a very bad memory.

* `/weights/theta_A.npy` is the default file to load the weights. Don't modify the variable named `ending=''` in `Diehl&Cook_spiking_MNIST.py` except you want to load the other checkpoint.



## Usage from original author
To run the simulations you also need to download the MNIST dataset from http://yann.lecun.com/exdb/mnist/ and install the brian simulator (the easiest way is to run the following command from a shell "easy_install brian", otherwise see http://briansimulator.org/docs/installation.html). 

Testing with pretrained weights:
First run the main file "Diehl&Cook_spiking_MNIST.py" (which by default uses the pretrained weights) and wait until the simulation finished (on recent hardware this should take about 30min). After the simulation finished, you can run "Diehl&Cook_MNIST_evaluation.py" to evaluate the result, which should show a performance of 91.56%.

Training a new network:
At first you have to modify the main file "Diehl&Cook_spiking_MNIST.py" by changing line 210 to "test_mode = False" to train the network. The resulting weights will be stored in the subfolder "weights". Those weights can then be used to test the performance of the network by running a simulation using "Diehl&Cook_spiking_MNIST.py" with line 210 changed back to "test_mode = True". After the simulation finished, the performance can be evaluated using "Diehl&Cook_MNIST_evaluation.py" and should be around 89% using the given parameters. 


If you have any questions don't hesitate to contact me via peter.u.diehl@gmail.com.



Note:
In this simple demo the performance is evaluated using neuron assignments of the test set. This leads to a slight increase in performance but violates good practice in machine learning. The results presented in the paper did NOT use the test set to determine neuron assignments. Instead the assignments were generated by running the same script in testing mode but using the 60000 examples of the training set to determine neuron assignments (this can be done by changing line 85 in "Diehl&Cook_MNIST_evaluation.py" to training_ending = '60000').
